## 支持向量机

### 原理

* 假设所有样本能在一个n维空间内被一个平面${w}^{T}x+b=0$（其中$w^{T}$是这个平面的法向量）划分，满足式子${w}^{T}x_{i}+b=1$或者${w}^{T}x_{i}+b=-1$的样本$x_{i}$称作支持向量。
* 两组支持向量构成的两个平面的距离为$\frac{2}{||w||}$。支持向量机目标是把这个值最优化。使用凸优化方法+拉格朗日乘子法+KKT条件可以得到对偶最优化表达式。

### 重要性质

* 重要性质1：从对偶最优化表达式可以看出，训练完成后大部分的训练样本不需保留，最终模型仅与支持向量有关。一般解法是SMO算法
* 重要性质2：如果原始空间是有限维，即属性数有限，那么一定存在一个更高维的特征空间使样本可分

### 核函数

根据重要性质2，如果在当前维度不能把样本分类，则需要借助更高维度的空间对样本做分类。假设有映射函数$x'=f(x)$把向量$x$映射到更高维的空间，并变成向量$x'$，把映射的向量代入对偶表达式，发现需要求向量$f(x_i)^{T}f(x_j)$的内积，这个开销会非常大。假设有一个函数$g(x_{i},x_{j})=f(x_i)^{T}f(x_j)$可以使用原向量就把内积求出来，那么开销就减小了很多。

我们定义$g(x_{i},x_{j})$为核函数。它隐式地定义了映射函数$f(x)$，让我们不需要具体的映射函数就可以在更高维解决支持向量机的划分问题。