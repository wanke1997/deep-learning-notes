## 决策树

### 决策树基本算法

输入的数据每一条训练数据`d`可以表示为：$\{a_{0}, a_{1}, ..., a_{n}\}: y$，代表这个数据有$n$个属性，分类结果是$y$。训练集`D`表示为$\{d_{1}, d_{2}, ..., d_{m}\}$，表示里面有m条训练数据。属性集A表示为数据集里面有n个属性，每条数据d有对属性的取值。

决策树的基本算法是递归建树。定义递归函数是`build_tree(D, A)`。base case有以下两种：

* 数据集D中所有的 $y$ 取值都相同，表示目前数据集中所有数据都是同一类。当前节点是叶子结点，取值为 $y$ 。
* 数据集D中所有的 $\{a_{0}, a_{1}, ..., a_{n}\}$ 取值全部相同，或者A为空，表示目前数据集所有数据属性全部相同。当前节点是叶子结点，取值为 $y$ 的众数

然后是general case。首先根据“某种算法”（下面讲）选出最优划分属性$\{a_{i}\}$。然后枚举$\{a_{i}\}$的所有取值，构建子树。同样有两种情况

* 当前$\{a_{i}\}$的取值$\{a_{i}^{v}\}$在数据集D中没有对应的样本d，生成的结点是叶子结点，取值为数据集D中 $y$ 的众数
* 当前数据集中$\{a_{i}\}$的取值等于$\{a_{i}^{v}\}$的样本们构建成数据集子集`D'`，同时集合A减去最优划分属性，做递归调用：`build_tree(D', A-ai)`

上述算法生成的结果就是决策树。

### 关于“某种算法”选择最优划分属性

* 定义信息熵$Ent(D)=-\sum_{k=1}^{|y|}p_{k}*log_{2}{p_{k}}$表示划分的混乱程度，信息熵越小划分纯度越高，反之则越低。注意信息熵公式中，$y$ 表示的是数据集分类结果的取值数量，p表示分类结果为k的样本占总样本的比例。
* 定义信息增益$Gain(D, a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^{v}|}{|D|}Ent(D^{v})$，其中$a$表示目前选取的划分属性，V表示划分属性$a$的所有取值。每个划分属性$a$都计算得到一个信息增益值。信息增益越大，信息纯度越高。
* 选取“信息增益”最大的属性$a$作为最优划分属性，就实现了ID3版本的决策树算法
* C4.5算法使用“增益率”来选取最优划分属性
* CART决策树使用“基尼指数”选择最优划分属性

### 剪枝

有两种剪枝方法：预剪枝（pre-pruning）和后剪枝（post-pruning）

* 预剪枝指的是在计算最优划分属性时，如果发现当前划分不能带来泛化性能提升，则停止划分并把当前节点置为叶子结点。具体实现是：对于一个属性，先计算【不做任何划分】时，验证集的正确率。然后【尝试做最优划分】，划分之后的子树使用各自的叶子结点代替，再次使用验证集去计算正确率。如果划分后的性能提升足够显著（大于某个阈值或显著高于不划分时的性能），则执行划分；否则不划分。
* 后剪枝指的是自底向上对非叶子结点考察，如果发现将当前节点替换成叶子结点能提升泛化性能，则将当前节点替换成叶子结点。具体实现是：把当前的非叶子结点使用验证集计算，得到正确率。然后【把当前节点变成叶子结点】（当前输入数据集D的标签y的众数）再次使用验证集去计算正确率，如果后者正确率比前者大，则执行剪枝；否则不执行。